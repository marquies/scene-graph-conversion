{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4540a450",
   "metadata": {},
   "source": [
    "# <center style='color:deeppink'>Calculate FID (`Frechet Inception Distance`) using PyTorch</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c0a24",
   "metadata": {},
   "source": [
    "# 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebac148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "Torchvision version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "print('Torchvision version:', torchvision.__version__)\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom module\n",
    "from INCEPTION import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39cafd",
   "metadata": {},
   "source": [
    "# 2. Define `ImagePathDataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2a2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTENSIONS = {'jpg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df4450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        \n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9961d",
   "metadata": {},
   "source": [
    "# 3. Define `get_activations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097823ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(files, model, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if batch_size > len(files):\n",
    "        batch_size = len(files)\n",
    "        \n",
    "    dataset = ImagePathDataset(files, transform=transforms.ToTensor())\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    pred_arr = np.empty((len(files), dims))\n",
    "    start_idx = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            pred = model(batch)[0]\n",
    "            \n",
    "        if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "            pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "        \n",
    "        pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "        pred_arr[start_idx:start_idx+pred.shape[0]] = pred\n",
    "        start_idx = start_idx + pred.shape[0]\n",
    "        \n",
    "    return pred_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf2939",
   "metadata": {},
   "source": [
    "# 4. Define `calculate_frechet_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22637133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, mu2, sigma1, sigma2, eps=1e-6):\n",
    "    \n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    \n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces sigular product; adding %s to diagonal cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "        \n",
    "    tr_covmean = np.trace(covmean)\n",
    "    \n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d63eb7",
   "metadata": {},
   "source": [
    "# 5. Define `calculate_activation_statistics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50baba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(files, model, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    act = get_activations(files, model, batch_size, dims, device)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    \n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250504c",
   "metadata": {},
   "source": [
    "# 6. Define `compute_statistics_of_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b953a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_of_path(path, model, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    path = pathlib.Path(path)\n",
    "    files = sorted([file for ext in IMAGE_EXTENSIONS for file in path.glob('*.{}'.format(ext))])\n",
    "    mu, sigma = calculate_activation_statistics(files, model, batch_size, dims, device)\n",
    "        \n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fc6f1",
   "metadata": {},
   "source": [
    "# 7. Calculate `FID distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41af0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid_given_paths(path1, path2, batch_size, dims, device='cpu'):\n",
    "    \n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "    model = InceptionV3([block_idx]).to(device)\n",
    "    \n",
    "    mu1, sigma1 = compute_statistics_of_path(path1, model, batch_size, dims, device)\n",
    "    mu2, sigma2 = compute_statistics_of_path(path2, model, batch_size, dims, device)\n",
    "    \n",
    "    fid_value = calculate_frechet_distance(mu1, mu2, sigma1, sigma2)\n",
    "    return print('FID distance:', round(fid_value, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8472971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in src_path: 781\n",
      "Total images in gen_path: 782\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 50\n",
    "dims = 2048\n",
    "\n",
    "src_path = '/Users/breucking/dev/fernuni/frameextractor/data/metaverse/original'\n",
    "gen_path = '/Users/breucking/dev/fernuni/frameextractor/data/metaverse/generated'\n",
    "\n",
    "print('Total images in src_path:', len(next(os.walk(src_path))[2]))\n",
    "print('Total images in gen_path:', len(next(os.walk(gen_path))[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c83ca6f-5423-4d51-9f20-15e3385645e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:30<00:00,  5.68s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:35<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID distance: 178.691\n"
     ]
    }
   ],
   "source": [
    "calculate_fid_given_paths(path1=src_path, path2=gen_path, batch_size=batch_size, dims=dims, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a053ea0",
   "metadata": {},
   "source": [
    "# A lower value of `FID distance` denotes more similarity with source images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
